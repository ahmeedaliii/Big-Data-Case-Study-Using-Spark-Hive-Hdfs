{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb786b58-d1a3-4b98-b8ec-7c19f9d5ead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session opened\n",
      "schema read\n",
      "will write to postgres now\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType, MapType\n",
    "\n",
    "# Function to create table if it doesn't exist\n",
    "def create_table_if_not_exists():\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS public.Top_Products (\n",
    "        window_start TIMESTAMP,\n",
    "        window_end TIMESTAMP,\n",
    "        productId VARCHAR,\n",
    "        count BIGINT\n",
    "    );\n",
    "    \"\"\"\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            database=\"postgres\",\n",
    "            user=\"postgres\",\n",
    "            password=\"itversity\",\n",
    "            host=\"spark-sql-and-pyspark-using-python3-cluster_util_db-1\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(create_table_query)\n",
    "        connection.commit()\n",
    "        cursor.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if connection is not None:\n",
    "            connection.close()\n",
    "\n",
    "# Run the function to create the table\n",
    "create_table_if_not_exists()\n",
    "\n",
    "# Initialize Spark session with necessary configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"streamtodb2\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.8\") \\\n",
    "    .config(\"spark.jars\", \"/data/project/postgresql-42.7.0.jar\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"session opened\")\n",
    "\n",
    "# Define common schema for JSON parsing\n",
    "schema = StructType([\n",
    "    StructField(\"eventType\", StringType(), True),\n",
    "    StructField(\"customerId\", StringType(), True),\n",
    "    StructField(\"productId\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),  # Change type to TimestampType\n",
    "    StructField(\"metadata\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"totalAmount\", FloatType(), True),\n",
    "    StructField(\"paymentMethod\", StringType(), True),\n",
    "    StructField(\"recommendedProductId\", StringType(), True),\n",
    "    StructField(\"algorithm\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"schema read\")\n",
    "\n",
    "# Kafka connection details\n",
    "bootstrap_servers = \"pkc-56d1g.eastus.azure.confluent.cloud:9092\"\n",
    "kafka_topic = \"sami_topic\"  # Add your topic name here\n",
    "kafka_username = \"JUKQQM4ZM632RECA\"\n",
    "kafka_password = \"UUkrPuSttgOC0U9lY3ZansNsKfN9fbxZPFwrGxudDrfv+knTD4rCwK+KdIzVPX0D\"\n",
    "\n",
    "# Read data from Kafka topic as a streaming DataFrame\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\",\n",
    "            f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_username}\" password=\"{kafka_password}\";') \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON data and select relevant fields\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING)\").select(from_json(\"value\", schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Filter product views\n",
    "product_views = parsed_df.filter(col(\"eventType\") == \"productView\") \\\n",
    "                         .select(\"eventType\", \"productId\", \"timestamp\")\n",
    "\n",
    "# Calculate top 5 products in the last 30 minutes\n",
    "top_products = product_views \\\n",
    "    .withWatermark(\"timestamp\", \"30 minute\") \\\n",
    "    .groupBy(window(col(\"timestamp\"), \"30 minute\"), col(\"productId\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(5)\n",
    "\n",
    "print(\"will write to postgres now\")\n",
    "\n",
    "# Write aggregated results (top products) to PostgreSQL\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    postgres_url = \"jdbc:postgresql://spark-sql-and-pyspark-using-python3-cluster_util_db-1:5432/postgres\"\n",
    "    postgres_properties = {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"itversity\",\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    batch_df.selectExpr(\"window.start as window_start\", \"window.end as window_end\", \"productId\", \"count\") \\\n",
    "        .write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .option(\"url\", postgres_url) \\\n",
    "        .option(\"dbtable\", \"public.Top_Products\") \\\n",
    "        .option(\"user\", postgres_properties[\"user\"]) \\\n",
    "        .option(\"password\", postgres_properties[\"password\"]) \\\n",
    "        .option(\"driver\", postgres_properties[\"driver\"]) \\\n",
    "        .save()\n",
    "\n",
    "query = top_products.writeStream \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d5e39-043b-49f5-8917-1549580bba41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
