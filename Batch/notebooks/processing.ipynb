{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c789e0ec-72c7-4379-a4cc-6f2b42b10ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, dayofmonth, month, year, quarter, substring_index, split, when, concat_ws, lit,round\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf5096e-d38c-4a34-87d8-534d00287ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"test\")\\\n",
    "    .master(\"yarn\")\\\n",
    "    .config(\"spark.submit.deployMode\", \"client\")\\\n",
    "    .config(\"spark.executor.instances\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .config(\"spark.eventLog.logBlockUpdates.enabled\", True)\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe555ad-2639-4d8c-a68d-8857e9e20f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current time and subtract one hour\n",
    "current_time = datetime.now()\n",
    "previous_hour_time = current_time - timedelta(hours=1)\n",
    "\n",
    "# Format the previous hour's directory path\n",
    "base_path = \"/retail_data/\"\n",
    "year = previous_hour_time.strftime(\"%Y\")\n",
    "month = previous_hour_time.strftime(\"%m\")\n",
    "day = previous_hour_time.strftime(\"%d\")\n",
    "hour = previous_hour_time.strftime(\"%H\")\n",
    "\n",
    "previous_hour_path = f\"{base_path}year={year}/month={month}/day={day}/hour={hour}\"\n",
    "\n",
    "# Define the filename patterns with wildcard for batch number\n",
    "branches_pattern = os.path.join(previous_hour_path, \"branches*.csv\")\n",
    "sales_agents_pattern = os.path.join(previous_hour_path, \"sales_agents*.csv\")\n",
    "sales_transactions_pattern = os.path.join(previous_hour_path, \"sales_transactions*.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b5c473-5c0e-4657-8e54-576764817f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------+-----+\n",
      "|branch_id|   location|establish_date|class|\n",
      "+---------+-----------+--------------+-----+\n",
      "|        1|   New York|    2017-01-15|    A|\n",
      "|        2|Los Angeles|    2016-07-28|    B|\n",
      "|        3|    Chicago|    2015-03-10|    A|\n",
      "|        4|    Houston|    2016-11-05|    D|\n",
      "|        5|    Phoenix|    2017-09-20|    C|\n",
      "+---------+-----------+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read branches CSV files into DataFrame\n",
    "\n",
    "branches_Dim = spark.read.csv(branches_pattern, header=True, inferSchema=True)\n",
    "\n",
    "# Change the data type of the establish_date column to DateType\n",
    "branches_Dim = branches_Dim.withColumn(\"establish_date\", col(\"establish_date\").cast(DateType()))\n",
    "branches_Dim.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8d0258-29d8-48f9-9872-d1ea9cb61fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+----------+\n",
      "|sales_person_id|              name| hire_date|\n",
      "+---------------+------------------+----------+\n",
      "|              1|          John Doe|2020-06-03|\n",
      "|              2|        Jane Smith|2018-05-13|\n",
      "|              3|   Michael Johnson|2021-10-03|\n",
      "|              4|       Emily Brown|2020-10-25|\n",
      "|              5|      David Wilson|2021-04-08|\n",
      "|              6|       Emma Taylor|2019-03-28|\n",
      "|              7|Christopher Miller|2020-01-11|\n",
      "|              8|      Olivia Davis|2021-10-24|\n",
      "|              9|   Daniel Martinez|2018-10-08|\n",
      "|             10|      Sophia Moore|2019-05-25|\n",
      "+---------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read sales agents CSV files into DataFrame\n",
    "\n",
    "sales_agents_Dim = spark.read.csv(sales_agents_pattern, header=True, inferSchema=True)\n",
    "\n",
    "# Change the data type of the value column to DateType\n",
    "sales_agents_Dim = sales_agents_Dim.withColumn(\"hire_date\", col(\"hire_date\").cast(DateType()))\n",
    "sales_agents_Dim.show(20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88ce09ab-70a5-4d17-93ba-f7e46f06f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sales transactions CSV files into DataFrame\n",
    "sales_transactions = spark.read.csv(sales_transactions_pattern, header=True, inferSchema=True)\n",
    "\n",
    "sales_transactions.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6846df28-dab1-4e40-8087-84f9d96b8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename some columns\n",
    "sales_transactions = sales_transactions.withColumnRenamed(\"cusomter_lname\", \"customer_lname\") \\\n",
    "                                       .withColumnRenamed(\"cusomter_email\", \"customer_email\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b57c7f7-85a6-4b11-a856-f2680f772785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Email column\n",
    "sales_transactions = sales_transactions.withColumn(\"customer_email\", \n",
    "                                                   substring_index(\"customer_email\", \".com\", 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d2d5126-e816-4b34-96d2-ff7266e25dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|customer_email        |\n",
      "+----------------------+\n",
      "|alexander.brown@gmail |\n",
      "|william.brown@gmail   |\n",
      "|john.williams@gmail   |\n",
      "|alexander.miller@yahoo|\n",
      "|john.brown@hotmail    |\n",
      "+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame to verify changes\n",
    "sales_transactions.select(\"customer_email\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e9feff7-ec2e-4ec9-adfb-779b5844584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns for customer_Dim DataFrame\n",
    "customer_Dim = sales_transactions.select(\n",
    "    col('customer_id'),\n",
    "    col('customer_fname'),\n",
    "    col('customer_lname'),\n",
    "    col('customer_email')\n",
    ").dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b96b12d-f02d-41c7-85d7-c2553e6e5b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------+--------------------------+\n",
      "|customer_id|customer_fname|customer_lname|customer_email            |\n",
      "+-----------+--------------+--------------+--------------------------+\n",
      "|85503      |Michael       |Jones         |michael.jones@yahoo       |\n",
      "|85497      |William       |Wilson        |william.wilson@yahoo      |\n",
      "|85506      |Ava           |Miller        |ava.miller@yahoo          |\n",
      "|85546      |Alexander     |Johnson       |alexander.johnson@gmail   |\n",
      "|85486      |James         |Johnson       |james.johnson@yahoo       |\n",
      "|85498      |Olivia        |Davis         |olivia.davis@hotmail      |\n",
      "|85487      |Mia           |Davis         |mia.davis@outlook         |\n",
      "|85561      |Alexander     |Moore         |alexander.moore@yahoo     |\n",
      "|85522      |John          |Brown         |john.brown@hotmail        |\n",
      "|85496      |James         |Davis         |james.davis@yahoo         |\n",
      "|85511      |Ava           |Moore         |ava.moore@gmail           |\n",
      "|85532      |Michael       |Brown         |michael.brown@yahoo       |\n",
      "|85475      |James         |Johnson       |james.johnson@gmail       |\n",
      "|85534      |James         |Wilson        |james.wilson@gmail        |\n",
      "|85539      |Emma          |Miller        |emma.miller@outlook       |\n",
      "|85510      |James         |Johnson       |james.johnson@outlook     |\n",
      "|85500      |John          |Brown         |john.brown@hotmail        |\n",
      "|85526      |Michael       |Miller        |michael.miller@outlook    |\n",
      "|85480      |Alexander     |Williams      |alexander.williams@outlook|\n",
      "|85464      |Emma          |Williams      |emma.williams@outlook     |\n",
      "+-----------+--------------+--------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the customer_Dim DataFrame\n",
    "customer_Dim.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12221831-64b6-4f0f-85a3-2d4a40163539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----+\n",
      "|location_id    |city        |state|\n",
      "+---------------+------------+-----+\n",
      "|Mesa-AZ        |Mesa        |AZ   |\n",
      "|Kenai-AK       |Kenai       |AK   |\n",
      "|Arvada-CO      |Arvada      |CO   |\n",
      "|Franklin-VT    |Franklin    |VT   |\n",
      "|Burlington-VT  |Burlington  |VT   |\n",
      "|Fayetteville-AR|Fayetteville|AR   |\n",
      "|Washington-DC  |Washington  |DC   |\n",
      "|Waddell-AZ     |Waddell     |AZ   |\n",
      "|Hartford-VT    |Hartford    |VT   |\n",
      "|Underhill-VT   |Underhill   |VT   |\n",
      "+---------------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Handle Location_Dim\n",
    "\n",
    "# Split the 'shipping_address' column\n",
    "split_col = split(sales_transactions['shipping_address'], '/')\n",
    "\n",
    "# Create 'city' and 'state' columns\n",
    "df = sales_transactions.withColumn('city', split_col.getItem(1)) \\\n",
    "                       .withColumn('state', split_col.getItem(2))\n",
    "\n",
    "# Select columns for location_Dim DataFrame\n",
    "df = df.select('city', 'state')\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Drop duplicate rows\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Create 'location_id' in the format 'city-state'\n",
    "df = df.withColumn(\"location_id\", concat_ws(\"-\", col(\"city\"), col(\"state\")))\n",
    "\n",
    "# Selecting columns for location_Dim DataFrame\n",
    "location_Dim = df.select(\n",
    "    col('location_id'),\n",
    "    col('city'),\n",
    "    col('state')\n",
    ")\n",
    "\n",
    "# Show the DataFrame\n",
    "location_Dim.show(10, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e13e4a-1194-489c-91c7-ad43c4df1593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+--------------+\n",
      "|payment_id     |is_online|payment_method|\n",
      "+---------------+---------+--------------+\n",
      "|yes-Credit Card|yes      |Credit Card   |\n",
      "|no-Cash        |no       |Cash          |\n",
      "|no-Credit Card |no       |Credit Card   |\n",
      "|yes-Stripe     |yes      |Stripe        |\n",
      "|yes-PayPal     |yes      |PayPal        |\n",
      "+---------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Handle Payment_Dim\n",
    "\n",
    "# Select columns for Payment_Dim DataFrame\n",
    "df = sales_transactions.select('is_online', 'payment_method')\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.na.drop()\n",
    "\n",
    "# Drop duplicate rows\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Create 'payment_id' in the format 'isonline-payment'\n",
    "df = df.withColumn(\"payment_id\", concat_ws(\"-\", col(\"is_online\"), col(\"payment_method\")))\n",
    "\n",
    "# Selecting columns for location_Dim DataFrame\n",
    "payment_Dim = df.select(\n",
    "    col('payment_id'),\n",
    "    col('is_online'),\n",
    "    col('payment_method')\n",
    ")\n",
    "\n",
    "# Show the DataFrame\n",
    "payment_Dim.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18432120-4374-4c8e-b80f-3d5f73fa994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns for product_Dim DataFrame\n",
    "product_Dim = sales_transactions.select(\"product_id\", \"product_name\", \"product_category\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad580a7b-598c-4337-bd97-e0cc4617eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------------+\n",
      "|product_id|product_name|product_category|\n",
      "+----------+------------+----------------+\n",
      "|        23|     Toaster|      Appliances|\n",
      "|         9|       Boots|        Footwear|\n",
      "|        14|      Camera|     Electronics|\n",
      "|         6|       Jeans|        Clothing|\n",
      "|         2|  Smartphone|     Electronics|\n",
      "+----------+------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_Dim.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2150a94-6e02-4829-8fd3-206fa209ec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|offer_id  |offer_name|percentage|\n",
      "+----------+----------+----------+\n",
      "|25-offer_5|offer_5   |25        |\n",
      "|15-offer_3|offer_3   |15        |\n",
      "|5-offer_1 |offer_1   |5         |\n",
      "|20-offer_4|offer_4   |20        |\n",
      "|10-offer_2|offer_2   |10        |\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Handle offer_Dim\n",
    "df = sales_transactions.select('offer_1', 'offer_2','offer_3','offer_4','offer_5')\n",
    "\n",
    "# Unpivot (melt) the DataFrame\n",
    "unpivot_expr = \"stack(5, 'offer_1', offer_1, 'offer_2', offer_2, 'offer_3', offer_3, 'offer_4', offer_4, 'offer_5', offer_5) as (offer_name, offer_value)\"\n",
    "unpivoted_df = df.selectExpr(unpivot_expr)\n",
    "\n",
    "# Filter out rows where offer_value is null\n",
    "unpivoted_df = unpivoted_df.filter(col(\"offer_value\").isNotNull())\n",
    "\n",
    "# Assign percentage based on the offer name\n",
    "unpivoted_df = unpivoted_df.withColumn(\n",
    "    \"percentage\",\n",
    "    when(col(\"offer_name\") == \"offer_1\", 5)\n",
    "    .when(col(\"offer_name\") == \"offer_2\", 10)\n",
    "    .when(col(\"offer_name\") == \"offer_3\", 15)\n",
    "    .when(col(\"offer_name\") == \"offer_4\", 20)\n",
    "    .when(col(\"offer_name\") == \"offer_5\", 25)\n",
    ")\n",
    "\n",
    "# Create offer_id by concatenating offer_name and percentage\n",
    "unpivoted_df = unpivoted_df.withColumn(\n",
    "    \"offer_id\",\n",
    "    concat_ws(\"-\",  col(\"percentage\"),col(\"offer_name\"))\n",
    ")\n",
    "\n",
    "# Select the required columns for offer_Dim DataFrame\n",
    "offer_Dim = unpivoted_df.select(\n",
    "    col('offer_id'),\n",
    "    col('offer_name').alias('offer_name'),\n",
    "    col('percentage')\n",
    ").distinct()\n",
    "\n",
    "# Show the offer_Dim DataFrame\n",
    "offer_Dim.show(10,truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f556101-aa08-411a-b30a-57607931f075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+-----+----+-------+---------+\n",
      "|     value| date_id|day|month|year|quarter| day_name|\n",
      "+----------+--------+---+-----+----+-------+---------+\n",
      "|2020-01-01|20200101|  1|    1|2020|      1|Wednesday|\n",
      "|2020-01-02|20200102|  2|    1|2020|      1| Thursday|\n",
      "|2020-01-03|20200103|  3|    1|2020|      1|   Friday|\n",
      "|2020-01-04|20200104|  4|    1|2020|      1| Saturday|\n",
      "|2020-01-05|20200105|  5|    1|2020|      1|   Sunday|\n",
      "|2020-01-06|20200106|  6|    1|2020|      1|   Monday|\n",
      "|2020-01-07|20200107|  7|    1|2020|      1|  Tuesday|\n",
      "|2020-01-08|20200108|  8|    1|2020|      1|Wednesday|\n",
      "|2020-01-09|20200109|  9|    1|2020|      1| Thursday|\n",
      "|2020-01-10|20200110| 10|    1|2020|      1|   Friday|\n",
      "|2020-01-11|20200111| 11|    1|2020|      1| Saturday|\n",
      "|2020-01-12|20200112| 12|    1|2020|      1|   Sunday|\n",
      "|2020-01-13|20200113| 13|    1|2020|      1|   Monday|\n",
      "|2020-01-14|20200114| 14|    1|2020|      1|  Tuesday|\n",
      "|2020-01-15|20200115| 15|    1|2020|      1|Wednesday|\n",
      "|2020-01-16|20200116| 16|    1|2020|      1| Thursday|\n",
      "|2020-01-17|20200117| 17|    1|2020|      1|   Friday|\n",
      "|2020-01-18|20200118| 18|    1|2020|      1| Saturday|\n",
      "|2020-01-19|20200119| 19|    1|2020|      1|   Sunday|\n",
      "|2020-01-20|20200120| 20|    1|2020|      1|   Monday|\n",
      "+----------+--------+---+-----+----+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Handle Date Dimension\n",
    "\n",
    "# Check if the retail_dwh path exists in HDFS to determine initial or incremental loading\n",
    "hdfs_path = \"/retail_dwh/date_Dim\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, date_format, dayofmonth, month, year, quarter, substring_index, split, when, concat_ws, lit,round\n",
    "    from pyspark.sql.types import DateType\n",
    "    from datetime import datetime, timedelta\n",
    "    import os\n",
    "\n",
    "    # Define the start and end dates\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    end_date = datetime(2030, 12, 31)\n",
    "\n",
    "    # Create a list of dates between start_date and end_date\n",
    "    date_list = [(start_date + timedelta(days=x)) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    # Convert the list of dates to a DataFrame\n",
    "    date_df = spark.createDataFrame([(date,) for date in date_list], [\"value\"])\n",
    "\n",
    "    # Add a date_id column formatted as 'yyyyMMdd'\n",
    "    date_Dim = date_df.withColumn(\n",
    "        \"date_id\",\n",
    "        date_format(col(\"value\"), \"yyyyMMdd\")\n",
    "    )\n",
    "\n",
    "    # Add day, month, year, quarter, and day_name columns\n",
    "    date_Dim = date_Dim.withColumn(\"day\", dayofmonth(col(\"value\"))) \\\n",
    "                       .withColumn(\"month\", month(col(\"value\"))) \\\n",
    "                       .withColumn(\"year\", year(col(\"value\"))) \\\n",
    "                       .withColumn(\"quarter\", quarter(col(\"value\"))) \\\n",
    "                       .withColumn(\"day_name\", date_format(col(\"value\"), \"EEEE\"))\n",
    "\n",
    "    # Change the data type of the value column to DateType\n",
    "    date_Dim = date_Dim.withColumn(\"value\", col(\"value\").cast(DateType()))\n",
    "    \n",
    "    date_Dim.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping the creation of the date dimension DataFrame.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b549a72d-8116-4441-b1a1-a9b0329e3075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>transaction_id</th><th>transaction_date_id</th><th>customer_id</th><th>sales_agent_id</th><th>branch_id</th><th>product_id</th><th>location_id</th><th>payment_id</th><th>offer_id</th><th>units</th><th>unit_price</th><th>total_price</th></tr>\n",
       "<tr><td>trx-152546429674</td><td>20230520</td><td>85469</td><td>1</td><td>2</td><td>22</td><td></td><td>no-Cash</td><td>null</td><td>10</td><td>79.99</td><td>799.9</td></tr>\n",
       "<tr><td>trx-291375327542</td><td>20221025</td><td>85512</td><td>3</td><td>1</td><td>24</td><td></td><td>no-Cash</td><td>20-offer_4</td><td>5</td><td>49.99</td><td>199.96</td></tr>\n",
       "<tr><td>trx-312507679871</td><td>20220205</td><td>85484</td><td>10</td><td>3</td><td>4</td><td></td><td>no-Credit Card</td><td>null</td><td>1</td><td>99.99</td><td>99.99</td></tr>\n",
       "<tr><td>trx-193384855491</td><td>20231020</td><td>85528</td><td>7</td><td>2</td><td>25</td><td></td><td>no-Cash</td><td>null</td><td>8</td><td>499.99</td><td>3999.92</td></tr>\n",
       "<tr><td>trx-831626097654</td><td>20221117</td><td>85500</td><td>5</td><td>1</td><td>14</td><td></td><td>no-Cash</td><td>15-offer_3</td><td>10</td><td>399.99</td><td>3399.915</td></tr>\n",
       "<tr><td>trx-158496122054</td><td>20220927</td><td>85545</td><td>4</td><td>5</td><td>14</td><td></td><td>no-Credit Card</td><td>25-offer_5</td><td>6</td><td>399.99</td><td>1799.955</td></tr>\n",
       "<tr><td>trx-722817999024</td><td>20220421</td><td>85561</td><td>4</td><td>1</td><td>30</td><td></td><td>no-Credit Card</td><td>20-offer_4</td><td>6</td><td>24.99</td><td>119.952</td></tr>\n",
       "<tr><td>trx-813287633702</td><td>20230428</td><td>85520</td><td>1</td><td>1</td><td>26</td><td></td><td>no-Cash</td><td>null</td><td>4</td><td>199.99</td><td>799.96</td></tr>\n",
       "<tr><td>trx-219568257432</td><td>20230308</td><td>85488</td><td>6</td><td>2</td><td>18</td><td></td><td>no-Credit Card</td><td>null</td><td>10</td><td>149.99</td><td>1499.9</td></tr>\n",
       "<tr><td>trx-352160720823</td><td>20230617</td><td>85466</td><td>5</td><td>2</td><td>16</td><td></td><td>no-Cash</td><td>null</td><td>8</td><td>39.99</td><td>319.92</td></tr>\n",
       "<tr><td>trx-895389231641</td><td>20220828</td><td>85559</td><td>6</td><td>5</td><td>14</td><td></td><td>no-Cash</td><td>null</td><td>6</td><td>399.99</td><td>2399.94</td></tr>\n",
       "<tr><td>trx-820309386661</td><td>20231119</td><td>85527</td><td>7</td><td>2</td><td>22</td><td></td><td>no-Credit Card</td><td>null</td><td>6</td><td>79.99</td><td>479.94</td></tr>\n",
       "<tr><td>trx-974983174642</td><td>20220322</td><td>85502</td><td>4</td><td>5</td><td>5</td><td></td><td>no-Cash</td><td>5-offer_1</td><td>10</td><td>19.99</td><td>189.905</td></tr>\n",
       "<tr><td>trx-119174604031</td><td>20220927</td><td>85508</td><td>1</td><td>3</td><td>9</td><td></td><td>no-Credit Card</td><td>null</td><td>1</td><td>129.99</td><td>129.99</td></tr>\n",
       "<tr><td>trx-474525094645</td><td>20220403</td><td>85549</td><td>2</td><td>3</td><td>2</td><td></td><td>no-Credit Card</td><td>null</td><td>9</td><td>699.99</td><td>6299.91</td></tr>\n",
       "<tr><td>trx-355633008132</td><td>20221214</td><td>85483</td><td>9</td><td>5</td><td>27</td><td></td><td>no-Credit Card</td><td>10-offer_2</td><td>9</td><td>29.99</td><td>242.919</td></tr>\n",
       "<tr><td>trx-490311940006</td><td>20230414</td><td>85496</td><td>8</td><td>2</td><td>19</td><td></td><td>no-Credit Card</td><td>10-offer_2</td><td>2</td><td>29.99</td><td>53.982</td></tr>\n",
       "<tr><td>trx-132205124356</td><td>20230119</td><td>85507</td><td>2</td><td>2</td><td>28</td><td></td><td>no-Cash</td><td>null</td><td>3</td><td>19.99</td><td>59.97</td></tr>\n",
       "<tr><td>trx-276319959382</td><td>20231014</td><td>85543</td><td>10</td><td>5</td><td>24</td><td></td><td>no-Cash</td><td>null</td><td>10</td><td>49.99</td><td>499.9</td></tr>\n",
       "<tr><td>trx-024853282614</td><td>20220324</td><td>85518</td><td>1</td><td>4</td><td>3</td><td></td><td>no-Cash</td><td>15-offer_3</td><td>1</td><td>299.99</td><td>254.992</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------------+-------------------+-----------+--------------+---------+----------+-----------+--------------+----------+-----+----------+-----------+\n",
       "|  transaction_id|transaction_date_id|customer_id|sales_agent_id|branch_id|product_id|location_id|    payment_id|  offer_id|units|unit_price|total_price|\n",
       "+----------------+-------------------+-----------+--------------+---------+----------+-----------+--------------+----------+-----+----------+-----------+\n",
       "|trx-152546429674|           20230520|      85469|             1|        2|        22|           |       no-Cash|      null|   10|     79.99|      799.9|\n",
       "|trx-291375327542|           20221025|      85512|             3|        1|        24|           |       no-Cash|20-offer_4|    5|     49.99|     199.96|\n",
       "|trx-312507679871|           20220205|      85484|            10|        3|         4|           |no-Credit Card|      null|    1|     99.99|      99.99|\n",
       "|trx-193384855491|           20231020|      85528|             7|        2|        25|           |       no-Cash|      null|    8|    499.99|    3999.92|\n",
       "|trx-831626097654|           20221117|      85500|             5|        1|        14|           |       no-Cash|15-offer_3|   10|    399.99|   3399.915|\n",
       "|trx-158496122054|           20220927|      85545|             4|        5|        14|           |no-Credit Card|25-offer_5|    6|    399.99|   1799.955|\n",
       "|trx-722817999024|           20220421|      85561|             4|        1|        30|           |no-Credit Card|20-offer_4|    6|     24.99|    119.952|\n",
       "|trx-813287633702|           20230428|      85520|             1|        1|        26|           |       no-Cash|      null|    4|    199.99|     799.96|\n",
       "|trx-219568257432|           20230308|      85488|             6|        2|        18|           |no-Credit Card|      null|   10|    149.99|     1499.9|\n",
       "|trx-352160720823|           20230617|      85466|             5|        2|        16|           |       no-Cash|      null|    8|     39.99|     319.92|\n",
       "|trx-895389231641|           20220828|      85559|             6|        5|        14|           |       no-Cash|      null|    6|    399.99|    2399.94|\n",
       "|trx-820309386661|           20231119|      85527|             7|        2|        22|           |no-Credit Card|      null|    6|     79.99|     479.94|\n",
       "|trx-974983174642|           20220322|      85502|             4|        5|         5|           |       no-Cash| 5-offer_1|   10|     19.99|    189.905|\n",
       "|trx-119174604031|           20220927|      85508|             1|        3|         9|           |no-Credit Card|      null|    1|    129.99|     129.99|\n",
       "|trx-474525094645|           20220403|      85549|             2|        3|         2|           |no-Credit Card|      null|    9|    699.99|    6299.91|\n",
       "|trx-355633008132|           20221214|      85483|             9|        5|        27|           |no-Credit Card|10-offer_2|    9|     29.99|    242.919|\n",
       "|trx-490311940006|           20230414|      85496|             8|        2|        19|           |no-Credit Card|10-offer_2|    2|     29.99|     53.982|\n",
       "|trx-132205124356|           20230119|      85507|             2|        2|        28|           |       no-Cash|      null|    3|     19.99|      59.97|\n",
       "|trx-276319959382|           20231014|      85543|            10|        5|        24|           |       no-Cash|      null|   10|     49.99|      499.9|\n",
       "|trx-024853282614|           20220324|      85518|             1|        4|         3|           |       no-Cash|15-offer_3|    1|    299.99|    254.992|\n",
       "+----------------+-------------------+-----------+--------------+---------+----------+-----------+--------------+----------+-----+----------+-----------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Handle transaction Fact table\n",
    "\n",
    "#extract the needed columns\n",
    "df = sales_transactions.drop(\"customer_fname\", \"customer_lname\",\"customer_email\",\"product_name\",\"product_category\")\n",
    "\n",
    "#change transaction_date data type to date\n",
    "df = df.withColumn(\"transaction_date\", col(\"transaction_date\").cast(\"date\"))\n",
    "\n",
    "#add transaction_date_id\n",
    "df = df.withColumn(\n",
    "    \"transaction_date_id\",\n",
    "    date_format(df[\"transaction_date\"], \"yyyyMMdd\")\n",
    ")\n",
    "\n",
    "#add location_id\n",
    "\n",
    "split_col = split(sales_transactions['shipping_address'], '/')\n",
    "\n",
    "df = df.withColumn('city', split_col.getItem(1)) \\\n",
    "                       .withColumn('state', split_col.getItem(2))\n",
    "\n",
    "df = df.withColumn(\"location_id\", concat_ws(\"-\", col(\"city\"), col(\"state\")))\n",
    "\n",
    "#add payment_id\n",
    "df = df.withColumn(\"payment_id\", concat_ws(\"-\", col(\"is_online\"), col(\"payment_method\")))\n",
    "\n",
    "#add offer_id\n",
    "\n",
    "offer_columns = ['offer_1', 'offer_2', 'offer_3', 'offer_4', 'offer_5']\n",
    "\n",
    "df = df.withColumn('offer_name',\n",
    "                   concat_ws(',',\n",
    "                             *[when(col(col_name), lit(col_name)).otherwise(None) for col_name in offer_columns]\n",
    "                             )\n",
    "                  )\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"percentage\",\n",
    "    when(col(\"offer_name\") == \"offer_1\", 5)\n",
    "    .when(col(\"offer_name\") == \"offer_2\", 10)\n",
    "    .when(col(\"offer_name\") == \"offer_3\", 15)\n",
    "    .when(col(\"offer_name\") == \"offer_4\", 20)\n",
    "    .when(col(\"offer_name\") == \"offer_5\", 25)\n",
    "    .otherwise(0) \n",
    ")\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"offer_id\",\n",
    "    when(col(\"percentage\") != 0, concat_ws(\"-\", col(\"percentage\"), col(\"offer_name\")))\n",
    "    .otherwise(None)  \n",
    ")\n",
    "\n",
    "#add total_price\n",
    "df = df.withColumn('total_price', round(col('units') * col('unit_price') * (1 - col('percentage') / 100), 3))\n",
    "\n",
    "sales_transactions_Fact = df.select(\n",
    "    col('transaction_id'),\n",
    "    col('transaction_date_id'),\n",
    "    col('customer_id'),\n",
    "    col('sales_agent_id'),\n",
    "    col('branch_id'),\n",
    "    col('product_id'),\n",
    "    col('location_id'),\n",
    "    col('payment_id'),\n",
    "    col('offer_id'),\n",
    "    col('units'),\n",
    "    col('unit_price'),\n",
    "    col('total_price'))\n",
    "\n",
    "sales_transactions_Fact\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e53250b-8392-4c03-99d0-2bedb0fe7037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create retail database\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac1b0e9d-5d1e-453e-a4ef-7bc9eb6c505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the sales_transactions_Fact into Hive\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sales_transactions_Fact = sales_transactions_Fact.withColumn(\"year\", F.substring(\"transaction_date_id\", 1, 4))\n",
    "sales_transactions_Fact = sales_transactions_Fact.withColumn(\"month\", F.substring(\"transaction_date_id\", 5, 2))\n",
    "sales_transactions_Fact = sales_transactions_Fact.withColumn(\"day\", F.substring(\"transaction_date_id\", 7, 2))\n",
    "\n",
    "# Check if the retail_dwh path exists in HDFS to determine initial or incremental loading\n",
    "hdfs_path = \"/retail_dwh/sales_transactions_Fact\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "\n",
    "    # Writing the DataFrame with partitioning\n",
    "    table_name = \"retail.sales_transactions_Fact\"\n",
    "    sales_transactions_Fact.coalesce(1).write.mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "      .option(\"path\", \"/retail_dwh/sales_transactions_Fact\") \\\n",
    "      .saveAsTable(table_name)\n",
    "    \n",
    "\n",
    "else:\n",
    "\n",
    "    # Writing the DataFrame with partitioning\n",
    "    table_name = \"retail.sales_transactions_Fact\"\n",
    "    sales_transactions_Fact.coalesce(1).write.mode(\"append\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "      .option(\"path\", \"/retail_dwh/sales_transactions_Fact\") \\\n",
    "      .saveAsTable(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df802238-75cd-4567-bc38-6cb2033043a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the branches_Dim into Hive\n",
    "\n",
    "# Check if the retail_dwh path exists in HDFS to determine initial or incremental loading\n",
    "\n",
    "hdfs_path = \"/retail_dwh/branches_Dim\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "\n",
    "    table_name = \"retail.branches_Dim\"\n",
    "    branches_Dim.write.mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/branches_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "\n",
    "else:\n",
    "    #apply incremental Loading\n",
    "    branches_Dim_i=spark.sql(\"select * from retail.branches_Dim\")\n",
    "    #Finding the new records only \n",
    "    branches_Dim_n = branches_Dim.join(branches_Dim_i, ['branch_id'], 'left_anti')\n",
    "   \n",
    "    table_name = \"retail.branches_Dim\"\n",
    "    branches_Dim_n.write.mode(\"append\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/branches_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5b5cabb0-c4d1-43a5-bda3-85ca83ac03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the sales_agents_Dim into Hive\n",
    "\n",
    "hdfs_path = \"/retail_dwh/sales_agents_Dim\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "    #apply incremental Loading\n",
    "    table_name = \"retail.sales_agents_Dim\"\n",
    "    sales_agents_Dim.write.mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/sales_agents_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "else:\n",
    "    #apply incremental Loading\n",
    "    sales_agents_Dim_i=spark.sql(\"select * from retail.sales_agents_Dim\")\n",
    "    #Finding the new records only \n",
    "    sales_agents_Dim_n = sales_agents_Dim.join(sales_agents_Dim_i,['sales_person_id'], 'left_anti')\n",
    "\n",
    "    table_name = \"retail.sales_agents_Dim\"\n",
    "    sales_agents_Dim_n.write.mode(\"append\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/sales_agents_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c767a70-7bfb-486e-8867-1ae5cbad54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"/retail_dwh/customer_Dim\"\n",
    "\n",
    "# Check if the HDFS path exists\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "    table_name = \"retail.customer_Dim\"\n",
    "    \n",
    "    \n",
    "    # Write the DataFrame to Hive as a single file\n",
    "    customer_Dim.coalesce(1).write.mode(\"overwrite\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", \"/retail_dwh/customer_Dim\") \\\n",
    "        .saveAsTable(table_name)\n",
    "else:\n",
    "    #apply incremental Loading\n",
    "    customer_Dim_i=spark.sql(\"select * from retail.customer_Dim\")\n",
    "    #Finding the new records only \n",
    "    customer_Dim_n = customer_Dim.join(customer_Dim_i,['customer_id'], 'left_anti')\n",
    "    \n",
    "    table_name = \"retail.customer_Dim\"\n",
    "    customer_Dim_n.write.mode(\"append\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/customer_Dim\") \\\n",
    "      .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ae305651-eb01-4768-aac6-403352bbfdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the location_Dim into Hive\n",
    "\n",
    "hdfs_path = \"/retail_dwh/location_Dim\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "    \n",
    "    table_name = \"retail.location_Dim\"\n",
    "    location_Dim.write.mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/location_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "else:\n",
    "    #apply incremental Loading\n",
    "    location_Dim_i=spark.sql(\"select * from retail.location_Dim\")\n",
    "    #Finding the new records only \n",
    "    location_Dim_n = location_Dim.join(location_Dim_i,['location_id'], 'left_anti')\n",
    "    \n",
    "    table_name = \"retail.location_Dim\"\n",
    "    location_Dim_n.write.mode(\"append\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/location_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "28f9d84b-b29c-47ed-bcdd-76da157055ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the payment_Dim into Hive\n",
    "\n",
    "hdfs_path = \"/retail_dwh/payment_Dim\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "    \n",
    "    table_name = \"retail.payment_Dim\"\n",
    "    payment_Dim.write.mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/payment_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "else:\n",
    "    #apply incremental Loading\n",
    "    payment_Dim_i=spark.sql(\"select * from retail.payment_Dim\")\n",
    "    #Finding the new records only \n",
    "    payment_Dim_n = payment_Dim.join(payment_Dim_i,['payment_id'], 'left_anti')\n",
    "    \n",
    "    table_name = \"retail.payment_Dim\"\n",
    "    payment_Dim_n.write.mode(\"append\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/payment_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8348ccae-18e6-4636-9285-7c4f5719a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the product_Dim into Hive\n",
    "\n",
    "hdfs_path = \"/retail_dwh/product_Dim\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "    \n",
    "    table_name = \"retail.product_Dim\"\n",
    "    product_Dim.write.mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/product_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "else:\n",
    "    product_Dim_i=spark.sql(\"select * from retail.product_Dim\")\n",
    "    #Finding the new records only \n",
    "    product_Dim_n = product_Dim.join(product_Dim_i,['product_id'], 'left_anti')\n",
    "    \n",
    "    table_name = \"retail.product_Dim\"\n",
    "    product_Dim_n.write.mode(\"append\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/product_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e142d7fc-2ee8-4ecf-88ba-454df072b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the offer_Dim into Hive\n",
    "\n",
    "hdfs_path = \"/retail_dwh/offer_Dim\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "    \n",
    "    table_name = \"retail.offer_Dim\"\n",
    "    offer_Dim.write.mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/offer_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "else:\n",
    "    offer_Dim_i=spark.sql(\"select * from retail.offer_Dim\")\n",
    "    #Finding the new records only \n",
    "    offer_Dim_n = offer_Dim.join(offer_Dim_i,['offer_id'], 'left_anti')\n",
    "    \n",
    "    table_name = \"retail.offer_Dim\"\n",
    "    offer_Dim_n.write.mode(\"append\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/offer_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec513d9-1f57-4b36-a986-c06363d7b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the date_Dim into Hive\n",
    "\n",
    "hdfs_path = \"/retail_dwh/date_Dim\"\n",
    "\n",
    "if not os.system(f\"hdfs dfs -test -d {hdfs_path}\") == 0:\n",
    "\n",
    "    table_name = \"retail.date_Dim\"\n",
    "    date_Dim.write.coalesce(1).mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .option(\"path\", \"/retail_dwh/date_Dim\") \\\n",
    "      .saveAsTable(table_name)\n",
    "else:\n",
    "    print(\"Skipping the writing of the date dimension.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fe80e1b-8857-4f4a-b274-d5d63f5ae93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
